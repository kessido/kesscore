{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "from kesscore.base import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class MultiActs(Module):\n",
    "    '''Given acts=[a0,a1] and v=[v0,v1] returns [a0(v0),a1(v0),a0(v1),a1(v1)]'''\n",
    "    def __init__(self, *acts): self.acts = nn.ModuleList(acts)\n",
    "    def forward(self, x): \n",
    "        if len(self.acts)==1: return self.acts[0](x)\n",
    "        return interleaved(*[f(x) for f in self.acts], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t,_r = [[-1,-2]],[[0,-1,0,-2]]\n",
    "test_eq(MultiActs(nn.ReLU(),nn.Identity())(torch.tensor(_t)),torch.tensor(_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def _listify(x): return x if isinstance(x, list) else [x]\n",
    "def _init_acts(acts, acts_params):\n",
    "    '''Init act using act_params. and combine using MultiActs\n",
    "    \n",
    "    _init_acts(nn.ReLU, {'inplace':True})\n",
    "    _init_acts([nn.ReLU,nn.ReLU], {'inplace':True})       #two relu with inplace\n",
    "    _init_acts([nn.ReLU,nn.ReLU], [{'inplace':True}, {}]) #two relu, first with inplace\n",
    "    _init_acts(nn.ReLU,           [{'inplace':True}, {}]) #two relu, first with inplace\n",
    "    '''\n",
    "    acts,acts_params = map(_listify, [acts, acts_params])\n",
    "\n",
    "    if len(acts)==0: return nn.Identity(),1\n",
    "    \n",
    "    n1,n2 = map(len, [acts, acts_params])\n",
    "    assert n1==n2 or 1 in [n1,n2], f'either equal or can be broadcast {[n1,n2]}'\n",
    "    acts = [act(**params) for act,params in zip_cycle_longest(acts,acts_params)]\n",
    "    \n",
    "    return MultiActs(*acts),len(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class Linear(Module):\n",
    "    __repr__=basic_repr('in_channels,out_channels,bias,groups')\n",
    "    def __init__(self, in_channels, out_channels, bias=True, groups=1):\n",
    "        store_attr()\n",
    "        if groups == 1: self._m = nn.Linear(in_channels, out_channels, bias)\n",
    "        else:           self._m = nn.Conv2d(in_channels, out_channels, 1, bias=bias, groups=groups)\n",
    "    def forward(self, x):\n",
    "        test_eq(x.ndim, 2)\n",
    "        def to2d(x): return x.view(*x.shape[:2])\n",
    "        def to4d(x): return x.view(*x.shape[:2], 1, 1)\n",
    "        fs = [to4d, self._m, to2d] if self.groups != 1 else [self._m]\n",
    "        return compose(*fs)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(repr(Linear(10,20,True)), 'Linear(in_channels=10, out_channels=20, bias=True, groups=1)')\n",
    "test_eq(Linear(10,20,groups=2)(torch.zeros(5,10)).shape, [5,20])\n",
    "test_eq(Linear(10,20)._m.weight.shape, [20,10])\n",
    "test_eq(Linear(10,20,groups=2)._m.weight.shape, [20,5,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def _linear_act_norm(c_in, c_out, *, is_final, groups, bias=True, bn=nn.BatchNorm1d, bn_params={}, acts=[nn.LeakyReLU], acts_params={}):\n",
    "    model = Linear(c_in, c_out, bias, groups)\n",
    "    if is_final: return model, c_in, c_out\n",
    "    acts, expansion = _init_acts(acts=acts, acts_params=acts_params)\n",
    "    bn = bn(c_out * expansion, **bn_params)\n",
    "    return nn.Sequential(model, acts, bn), c_in, c_out * expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def _build_channels(*, c_in, c_mid, c_out, n_layers):\n",
    "    assert n_layers >= 1\n",
    "    return [c_in] + [c_mid]*(n_layers-1) + [c_out]\n",
    "def _build_heads(*, channels, heads, groups, in_groups):\n",
    "    assert groups == in_groups == 1,'if you use head, dont touch groups'\n",
    "    channels = channels[:1] + [c*heads for c in channels[1:]]\n",
    "    groups = heads\n",
    "    return channels,groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@delegates(_linear_act_norm, but='groups')\n",
    "def MLP(*, c_in=None, c_mid=None, c_out=None, n_layers=None, channels=None, groups=1, in_groups=1, heads=None, bias_last=True, **kwargs):\n",
    "    test_all_eq(c_in, c_mid, c_out, n_layers, map=isinstance(NoneType))\n",
    "    assert (c_in is None) != (channels is None),'either channels of in\\\\mid\\\\out\\\\nlayers'\n",
    "    if in_groups != 1: assert in_groups == groups, 'unexpected usage'\n",
    "    if c_in is not None: channels = _build_channels(c_in=c_in, c_mid=c_mid, c_out=c_out, n_layers=n_layers)\n",
    "    if heads is not None: channels,groups = _build_heads(channels=channels, heads=heads, groups=groups, in_groups=in_groups)\n",
    "    assert len(channels) >= 2\n",
    "    blocks,c_in,g = OrderedDict(),channels[0],in_groups\n",
    "    for i,c_out in enumerate(channels[1:-1]):\n",
    "        m,_,c_in = _linear_act_norm(c_in,c_out,groups=g,is_final=False,**kwargs)\n",
    "        g = groups\n",
    "        blocks[f'block_{i}'] = m\n",
    "    if bias_last is not None: kwargs['bias'] = bias_last\n",
    "    head,*_ = _linear_act_norm(c_in,channels[-1],groups=in_groups,is_final=True,**kwargs)\n",
    "    blocks['head'] = head\n",
    "    return nn.Sequential(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(channels=[10,20,30], bn=nn.BatchNorm1d, bn_params={'affine':False}, \n",
    "          acts=[nn.ReLU, nn.LeakyReLU], acts_params=[{},{'negative_slope':1e-3}])\n",
    "test_eq(len(mlp), 2)\n",
    "test_fail(lambda:mlp(torch.zeros(10,20)), contains='size mismatch')\n",
    "test_eq(mlp    (torch.zeros(10,10)).shape, [10, 30])\n",
    "test_eq(mlp[:1](torch.zeros(10,10)).shape, [10, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(channels=[10,20,20,30], bn=lambda x:nn.Identity(), heads=3)\n",
    "inp  = torch.ones(1,10)\n",
    "mres = mlp[:2](inp)\n",
    "test_eq(mres.shape, [1,60])\n",
    "res = mlp[2:](mres)\n",
    "test_eq(res.shape, [1,90])\n",
    "grad = torch.autograd.grad(res[0,0], mres)[0]\n",
    "grad_t,grad_f = grad[0, :20],grad[0, 20:]\n",
    "assert not (grad_t==0).any()\n",
    "assert     (grad_f==0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(c_in=10, c_mid=20, c_out=30, n_layers=2, heads=10, bn=nn.BatchNorm1d, bn_params={'affine':False}, \n",
    "          acts=[nn.ReLU, nn.LeakyReLU], acts_params=[{},{'negative_slope':1e-3}])\n",
    "test_eq(len(mlp), 2)\n",
    "test_fail(lambda:mlp(torch.zeros(10,20)), contains='size mismatch')\n",
    "test_eq(mlp    (torch.zeros(10,10)).shape, [10, 300])\n",
    "test_eq(mlp[:1](torch.zeros(10,10)).shape, [10, 200*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_functional.ipynb.\n",
      "Converted 01_tests.ipynb.\n",
      "Converted 02_tensor.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 04_random.ipynb.\n",
      "Converted 05_domainadaptation.ipynb.\n",
      "Converted 06_mlp.ipynb.\n",
      "Converted 07_download.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.sync import notebook2script;notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "from kesscore.imports import *\n",
    "from kesscore.functional import *\n",
    "from kesscore.tensor import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class MultiActs(nn.Sequential):\n",
    "    '''Given acts=[a0,a1] and v=[v0,v1] returns [a0(v0),a1(v0),a0(v1),a1(v1)]'''\n",
    "    def forward(self, x): return interleaved([f(x) for f in self], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t,_r = [[-1,-2]],[[0,-1,0,-2]]\n",
    "test_eq(MultiActs(nn.ReLU(), nn.Identity())(torch.tensor(_t)),torch.tensor(_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def _init_acts(acts, acts_params):\n",
    "    '''Init act using act_params. in case one of them is a list with multiple items, somewhat of a broadcast is used, and than combined using MultiAct'''\n",
    "    if not isinstance(acts,        list): acts        = [acts]\n",
    "    if not isinstance(acts_params, list): acts_params = [acts_params]\n",
    "    na,nap = len(acts),len(acts_params)\n",
    "    assert na==nap or 1 in [na,nap], f'either equal or one is eqaul to 1 {[na,nap]}'\n",
    "    \n",
    "    acts = [act(**params) for act,params in zip_cycle_longest(acts,acts_params)]\n",
    "    if   len(acts)==0: return nn.Identity(), 1\n",
    "    elif len(acts)==1: return acts[0], 1\n",
    "    else:              return MultiActs(*acts), len(acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class Linear(Module):\n",
    "    __repr__=basic_repr('in_channels,out_channels,bias,groups')\n",
    "    def __init__(self, in_channels, out_channels, bias=True, groups=1):\n",
    "        store_attr()\n",
    "        if groups == 1: self.m = nn.Linear(in_channels, out_channels, bias)\n",
    "        else:           self.m = nn.Conv2d(in_channels, out_channels, 1, bias=bias, groups=groups)\n",
    "    def forward(self, x):\n",
    "        test_eq(x.ndim, 2)\n",
    "        if self.groups == 1: return self.m(x)\n",
    "        x = x.view(*x.shape, 1, 1)\n",
    "        x = self.m(x)\n",
    "        return x.view(*x.shape[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(repr(Linear(10,20,True)), 'Linear(in_channels=10, out_channels=20, bias=True, groups=1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def _linear_act_norm(c_in, c_out, *, is_final, groups, bias=True, bn=nn.BatchNorm1d, bn_params={}, acts=[nn.LeakyReLU], acts_params={}):\n",
    "    model = Linear(c_in, c_out, bias, groups)\n",
    "    if is_final: return model, c_in, c_out\n",
    "    acts, expansion = _init_acts(acts=acts, acts_params=acts_params)\n",
    "    bn = bn(c_out * expansion, **bn_params)\n",
    "    return nn.Sequential(nn.Linear(c_in, c_out, bias), acts, bn), c_in, c_out * expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "@delegates(_linear_act_norm, but='groups')\n",
    "def MLP(*, c_in=None, c_mid=None, c_out=None, n_layers=None, channels=None, groups=1, in_groups=1, heads=None, **kwargs):\n",
    "    L(c_in, c_mid, c_out, n_layers).map(isinstance(NoneType)).assert_all_eq()\n",
    "    assert (c_in is None) != (channels is None),'either channels of in\\\\mid\\\\out\\\\nlayers'\n",
    "    if c_in is not None: \n",
    "        assert n_layers >= 1\n",
    "        channels = [c_in] + [c_mid]*(n_layers-1) + [c_out]\n",
    "    assert len(channels) >= 2\n",
    "    if in_groups != 1: assert in_groups == groups, 'unexpected usage'\n",
    "    if heads is not None:\n",
    "        assert groups == in_groups == 1, 'if you use head, dont touch groups'\n",
    "        groups = heads\n",
    "        channels = channels[:1] + [c*heads for c in channels[1:]]\n",
    "    blocks,c_in = OrderedDict(),channels[0]\n",
    "    for i,c_out in enumerate(channels[1:-1]):\n",
    "        m,_,c_in = _linear_act_norm(c_in,c_out,groups=in_groups,is_final=False,**kwargs)\n",
    "        in_groups = groups\n",
    "        blocks[f'block_{i}'] = m\n",
    "    head,*_ = _linear_act_norm(c_in,channels[-1],groups=in_groups,is_final=True,**kwargs)\n",
    "    blocks['head'] = head\n",
    "    return nn.Sequential(blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(channels=[10,20,30], bn=nn.BatchNorm1d, bn_params={'affine':False}, \n",
    "          acts=[nn.ReLU, nn.LeakyReLU], acts_params=[{},{'negative_slope':1e-3}])\n",
    "test_eq(len(mlp), 2)\n",
    "test_fail(lambda:mlp(torch.zeros(10,20)), contains='size mismatch, m1: [10 x 20], m2: [10 x 20]')\n",
    "test_eq(mlp    (torch.zeros(10,10)).shape, [10, 30])\n",
    "test_eq(mlp[:1](torch.zeros(10,10)).shape, [10, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(c_in=10, c_mid=20, c_out=30, n_layers=2, heads=10, bn=nn.BatchNorm1d, bn_params={'affine':False}, \n",
    "          acts=[nn.ReLU, nn.LeakyReLU], acts_params=[{},{'negative_slope':1e-3}])\n",
    "test_eq(len(mlp), 2)\n",
    "test_fail(lambda:mlp(torch.zeros(10,20)), contains='size mismatch, m1: [10 x 20], m2: [10 x 200]')\n",
    "test_eq(mlp    (torch.zeros(10,10)).shape, [10, 300])\n",
    "test_eq(mlp[:1](torch.zeros(10,10)).shape, [10, 200*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.sync import notebook2script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_functional.ipynb.\n",
      "Converted 01_images.ipynb.\n",
      "Converted 02_download.ipynb.\n",
      "Converted 03_tensor.ipynb.\n",
      "Converted 04_random.ipynb.\n",
      "Converted 05_domainadaptation.ipynb.\n",
      "Converted 06_mlp.ipynb.\n",
      "Converted 07_tests.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai] *",
   "language": "python",
   "name": "conda-env-fastai-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

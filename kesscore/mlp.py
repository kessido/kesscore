# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/06_mlp.ipynb (unless otherwise specified).

__all__ = ['MultiActs', 'Linear', 'MLP']

# Cell
from .imports import *
from .functional import *
from .tensor import *
import copy

# Cell
class MultiActs(nn.Sequential):
    '''Given acts=[a0,a1] and v=[v0,v1] returns [a0(v0),a1(v0),a0(v1),a1(v1)]'''
    def forward(self, x): return interleaved([f(x) for f in self], dim=1)

# Cell
def _init_acts(acts, acts_params):
    '''Init act using act_params. in case one of them is a list with multiple items, somewhat of a broadcast is used, and than combined using MultiAct'''
    if not isinstance(acts,        list): acts        = [acts]
    if not isinstance(acts_params, list): acts_params = [acts_params]
    na,nap = len(acts),len(acts_params)
    assert na==nap or 1 in [na,nap], f'either equal or one is eqaul to 1 {[na,nap]}'

    acts = [act(**params) for act,params in zip_cycle_longest(acts,acts_params)]
    if   len(acts)==0: return nn.Identity(), 1
    elif len(acts)==1: return acts[0], 1
    else:              return MultiActs(*acts), len(acts)

# Cell
class Linear(Module):
    __repr__=basic_repr('in_channels,out_channels,bias,groups')
    def __init__(self, in_channels, out_channels, bias=True, groups=1):
        store_attr()
        if groups == 1: self.m = nn.Linear(in_channels, out_channels, bias)
        else:           self.m = nn.Conv2d(in_channels, out_channels, 1, bias=bias, groups=groups)
    def forward(self, x):
        test_eq(x.ndim, 2)
        if self.groups == 1: return self.m(x)
        x = x.view(*x.shape, 1, 1)
        x = self.m(x)
        return x.view(*x.shape[:2])

# Cell
def _linear_act_norm(c_in, c_out, *, is_final, groups, bias=True, bn=nn.BatchNorm1d, bn_params={}, acts=[nn.LeakyReLU], acts_params={}):
    model = Linear(c_in, c_out, bias, groups)
    if is_final: return model, c_in, c_out
    acts, expansion = _init_acts(acts=acts, acts_params=acts_params)
    bn = bn(c_out * expansion, **bn_params)
    return nn.Sequential(nn.Linear(c_in, c_out, bias), acts, bn), c_in, c_out * expansion

# Cell
@delegates(_linear_act_norm, but='groups')
def MLP(*, c_in=None, c_mid=None, c_out=None, n_layers=None, channels=None, groups=1, in_groups=1, heads=None, **kwargs):
    L(c_in, c_mid, c_out, n_layers).map(isinstance(NoneType)).assert_all_eq()
    assert (c_in is None) != (channels is None),'either channels of in\\mid\\out\\nlayers'
    if c_in is not None:
        assert n_layers >= 1
        channels = [c_in] + [c_mid]*(n_layers-1) + [c_out]
    assert len(channels) >= 2
    if in_groups != 1: assert in_groups == groups, 'unexpected usage'
    if heads is not None:
        assert groups == in_groups == 1, 'if you use head, dont touch groups'
        groups = heads
        channels = channels[:1] + [c*heads for c in channels[1:]]
    blocks,c_in = OrderedDict(),channels[0]
    for i,c_out in enumerate(channels[1:-1]):
        m,_,c_in = _linear_act_norm(c_in,c_out,groups=in_groups,is_final=False,**kwargs)
        in_groups = groups
        blocks[f'block_{i}'] = m
    head,*_ = _linear_act_norm(c_in,channels[-1],groups=in_groups,is_final=True,**kwargs)
    blocks['head'] = head
    return nn.Sequential(blocks)